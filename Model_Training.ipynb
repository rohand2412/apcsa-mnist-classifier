{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AMC_Model_Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGbIUGmQ4N4X",
        "outputId": "ddc1ed94-f5d0-42e4-94d4-53a33939c7f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# import Tensorflow, Numpy, Matplotlib, and mount my google drive\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "print(\"X_train shape\", X_train.shape)\n",
        "print(\"y_train shape\", y_train.shape)\n",
        "print(\"X_test shape\", X_test.shape)\n",
        "print(\"y_test shape\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ultGLyr5poj",
        "outputId": "2823cde4-d50d-4d5d-d7e4-54b2e77fe890"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape (60000, 28, 28)\n",
            "y_train shape (60000,)\n",
            "X_test shape (10000, 28, 28)\n",
            "y_test shape (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a training image\n",
        "plt.imshow(X_train[0],cmap='gray', vmin = 0, vmax = 255)\n",
        "print(X_train[0, 0, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "pSEHnk1UE6uK",
        "outputId": "5ac9c754-f0ba-4bc3-8a0f-71628ea7d207"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAN9klEQVR4nO3df4xV9ZnH8c+zWP6QojBrOhKKSyEGg8ZON4gbl6w1hvojGhw1TSexoZE4/YNJaLIhNewf1WwwZBU2SzTNTKMWNl1qEzUgaQouoOzGhDgiKo5LdQ2mTEaowZEf/mCHefaPezBTnfu9w7nn3nOZ5/1Kbu6957nnnicnfDi/7pmvubsATH5/VXYDAJqDsANBEHYgCMIOBEHYgSAuaubCzIxT/0CDubuNN72uLbuZ3Wpmh8zsPTN7sJ7vAtBYlvc6u5lNkfRHSUslHZH0qqQudx9IzMOWHWiwRmzZF0t6z93fd/czkn4raVkd3weggeoJ+2xJfxrz/kg27S+YWbeZ9ZtZfx3LAlCnhp+gc/c+SX0Su/FAmerZsg9KmjPm/bezaQBaUD1hf1XSlWb2HTObKulHkrYV0xaAouXejXf3ETPrkbRD0hRJT7n724V1BqBQuS+95VoYx+xAwzXkRzUALhyEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBJF7yGZcGKZMmZKsX3rppQ1dfk9PT9XaxRdfnJx3wYIFyfrKlSuT9ccee6xqraurKznv559/nqyvW7cuWX/44YeT9TLUFXYzOyzppKSzkkbcfVERTQEoXhFb9pvc/aMCvgdAA3HMDgRRb9hd0k4ze83Musf7gJl1m1m/mfXXuSwAdah3N36Juw+a2bckvWhm/+Pue8d+wN37JPVJkpl5ncsDkFNdW3Z3H8yej0l6XtLiIpoCULzcYTezaWY2/dxrST+QdLCoxgAUq57d+HZJz5vZue/5D3f/QyFdTTJXXHFFsj516tRk/YYbbkjWlyxZUrU2Y8aM5Lz33HNPsl6mI0eOJOsbN25M1js7O6vWTp48mZz3jTfeSNZffvnlZL0V5Q67u78v6bsF9gKggbj0BgRB2IEgCDsQBGEHgiDsQBDm3rwftU3WX9B1dHQk67t3707WG32baasaHR1N1u+///5k/dSpU7mXPTQ0lKx//PHHyfqhQ4dyL7vR3N3Gm86WHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4Dp7Adra2pL1ffv2Jevz5s0rsp1C1ep9eHg4Wb/pppuq1s6cOZOcN+rvD+rFdXYgOMIOBEHYgSAIOxAEYQeCIOxAEIQdCIIhmwtw/PjxZH316tXJ+h133JGsv/7668l6rT+pnHLgwIFkfenSpcn66dOnk/Wrr766am3VqlXJeVEstuxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EAT3s7eASy65JFmvNbxwb29v1dqKFSuS8953333J+pYtW5J1tJ7c97Ob2VNmdszMDo6Z1mZmL5rZu9nzzCKbBVC8iezG/1rSrV+Z9qCkXe5+paRd2XsALaxm2N19r6Sv/h50maRN2etNku4quC8ABcv72/h2dz83WNaHktqrfdDMuiV151wOgILUfSOMu3vqxJu790nqkzhBB5Qp76W3o2Y2S5Ky52PFtQSgEfKGfZuk5dnr5ZK2FtMOgEapuRtvZlskfV/SZWZ2RNIvJK2T9DszWyHpA0k/bGSTk92JEyfqmv+TTz7JPe8DDzyQrD/zzDPJeq0x1tE6aobd3buqlG4uuBcADcTPZYEgCDsQBGEHgiDsQBCEHQiCW1wngWnTplWtvfDCC8l5b7zxxmT9tttuS9Z37tyZrKP5GLIZCI6wA0EQdiAIwg4EQdiBIAg7EARhB4LgOvskN3/+/GR9//79yfrw8HCyvmfPnmS9v7+/au2JJ55IztvMf5uTCdfZgeAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIrrMH19nZmaw//fTTyfr06dNzL3vNmjXJ+ubNm5P1oaGhZD0qrrMDwRF2IAjCDgRB2IEgCDsQBGEHgiDsQBBcZ0fSNddck6xv2LAhWb/55vyD/fb29ibra9euTdYHBwdzL/tClvs6u5k9ZWbHzOzgmGkPmdmgmR3IHrcX2SyA4k1kN/7Xkm4dZ/q/untH9vh9sW0BKFrNsLv7XknHm9ALgAaq5wRdj5m9me3mz6z2ITPrNrN+M6v+x8gANFzesP9S0nxJHZKGJK2v9kF373P3Re6+KOeyABQgV9jd/ai7n3X3UUm/krS42LYAFC1X2M1s1pi3nZIOVvssgNZQ8zq7mW2R9H1Jl0k6KukX2fsOSS7psKSfunvNm4u5zj75zJgxI1m/8847q9Zq3StvNu7l4i/t3r07WV+6dGmyPllVu85+0QRm7Bpn8pN1dwSgqfi5LBAEYQeCIOxAEIQdCIKwA0FwiytK88UXXyTrF12Uvlg0MjKSrN9yyy1Vay+99FJy3gsZf0oaCI6wA0EQdiAIwg4EQdiBIAg7EARhB4KoedcbYrv22muT9XvvvTdZv+6666rWal1Hr2VgYCBZ37t3b13fP9mwZQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBILjOPsktWLAgWe/p6UnW77777mT98ssvP++eJurs2bPJ+tBQ+q+Xj46OFtnOBY8tOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwXX2C0Cta9ldXeMNtFtR6zr63Llz87RUiP7+/mR97dq1yfq2bduKbGfSq7llN7M5ZrbHzAbM7G0zW5VNbzOzF83s3ex5ZuPbBZDXRHbjRyT9o7svlPR3klaa2UJJD0ra5e5XStqVvQfQomqG3d2H3H1/9vqkpHckzZa0TNKm7GObJN3VqCYB1O+8jtnNbK6k70naJ6nd3c/9OPlDSe1V5umW1J2/RQBFmPDZeDP7pqRnJf3M3U+MrXlldMhxB2109z53X+Tui+rqFEBdJhR2M/uGKkH/jbs/l00+amazsvosScca0yKAItTcjTczk/SkpHfcfcOY0jZJyyWty563NqTDSaC9fdwjnC8tXLgwWX/88ceT9auuuuq8eyrKvn37kvVHH320am3r1vQ/GW5RLdZEjtn/XtKPJb1lZgeyaWtUCfnvzGyFpA8k/bAxLQIoQs2wu/t/Sxp3cHdJNxfbDoBG4eeyQBCEHQiCsANBEHYgCMIOBMEtrhPU1tZWtdbb25uct6OjI1mfN29erp6K8MorryTr69evT9Z37NiRrH/22Wfn3RMagy07EARhB4Ig7EAQhB0IgrADQRB2IAjCDgQR5jr79ddfn6yvXr06WV+8eHHV2uzZs3P1VJRPP/20am3jxo3JeR955JFk/fTp07l6Quthyw4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQYS5zt7Z2VlXvR4DAwPJ+vbt25P1kZGRZD11z/nw8HByXsTBlh0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgjB3T3/AbI6kzZLaJbmkPnf/NzN7SNIDkv6cfXSNu/++xnelFwagbu4+7qjLEwn7LEmz3H2/mU2X9Jqku1QZj/2Uuz820SYIO9B41cI+kfHZhyQNZa9Pmtk7ksr90ywAztt5HbOb2VxJ35O0L5vUY2ZvmtlTZjazyjzdZtZvZv11dQqgLjV347/8oNk3Jb0saa27P2dm7ZI+UuU4/p9V2dW/v8Z3sBsPNFjuY3ZJMrNvSNouaYe7bxinPlfSdne/psb3EHagwaqFveZuvJmZpCclvTM26NmJu3M6JR2st0kAjTORs/FLJP2XpLckjWaT10jqktShym78YUk/zU7mpb6LLTvQYHXtxheFsAONl3s3HsDkQNiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQii2UM2fyTpgzHvL8umtaJW7a1V+5LoLa8ie/ubaoWm3s/+tYWb9bv7otIaSGjV3lq1L4ne8mpWb+zGA0EQdiCIssPeV/LyU1q1t1btS6K3vJrSW6nH7ACap+wtO4AmIexAEKWE3cxuNbNDZvaemT1YRg/VmNlhM3vLzA6UPT5dNobeMTM7OGZam5m9aGbvZs/jjrFXUm8Pmdlgtu4OmNntJfU2x8z2mNmAmb1tZquy6aWuu0RfTVlvTT9mN7Mpkv4oaamkI5JeldTl7gNNbaQKMzssaZG7l/4DDDP7B0mnJG0+N7SWmf2LpOPuvi77j3Kmu/+8RXp7SOc5jHeDeqs2zPhPVOK6K3L48zzK2LIvlvSeu7/v7mck/VbSshL6aHnuvlfS8a9MXiZpU/Z6kyr/WJquSm8twd2H3H1/9vqkpHPDjJe67hJ9NUUZYZ8t6U9j3h9Ra4337pJ2mtlrZtZddjPjaB8zzNaHktrLbGYcNYfxbqavDDPeMusuz/Dn9eIE3dctcfe/lXSbpJXZ7mpL8soxWCtdO/2lpPmqjAE4JGl9mc1kw4w/K+ln7n5ibK3MdTdOX01Zb2WEfVDSnDHvv51NawnuPpg9H5P0vCqHHa3k6LkRdLPnYyX38yV3P+ruZ919VNKvVOK6y4YZf1bSb9z9uWxy6etuvL6atd7KCPurkq40s++Y2VRJP5K0rYQ+vsbMpmUnTmRm0yT9QK03FPU2Scuz18slbS2xl7/QKsN4VxtmXCWvu9KHP3f3pj8k3a7KGfn/lfRPZfRQpa95kt7IHm+X3ZukLars1v2fKuc2Vkj6a0m7JL0r6T8ltbVQb/+uytDeb6oSrFkl9bZElV30NyUdyB63l73uEn01Zb3xc1kgCE7QAUEQdiAIwg4EQdiBIAg7EARhB4Ig7EAQ/w8ie3GmjcGk5QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Invert the dataset\n",
        "X_train = 255 - X_train\n",
        "X_test = 255 - X_test"
      ],
      "metadata": {
        "id": "WV7QlKvpJAvl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the same training image as sanity check\n",
        "plt.imshow(X_train[0],cmap='gray', vmin = 0, vmax = 255)\n",
        "print(X_train[0, 0, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "cbZtEdwsJDOo",
        "outputId": "5b1b50fa-8c1d-4ffc-e31b-4bc84c9ba140"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "255\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a dimension to the end of the train and test sets\n",
        "X_train = np.expand_dims(X_train, axis=-1)\n",
        "X_test = np.expand_dims(X_test, axis=-1)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "vqHAncA29t38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b65c8f6-c8ac-482c-8f95-711a0ff56068"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility variables\n",
        "dropout = 0.2\n",
        "conv = 3\n",
        "affine = 2"
      ],
      "metadata": {
        "id": "KvlqEVBY-ZBL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate and Regularization optimization\n",
        "for _ in range(5):\n",
        "\n",
        "  # Randomly select a learning rate\n",
        "  lr = 10**np.random.uniform(-4, -2.3)\n",
        "\n",
        "  # Randomly select a regularization value\n",
        "  reg = 10**np.random.uniform(-5, -4)\n",
        "\n",
        "  # Print them out\n",
        "  print(\"lr: %f reg: %f\" % (lr, reg))\n",
        "\n",
        "  # Define the model as Sequential\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Define the input\n",
        "  model.add(tf.keras.layers.Input(shape=X_train.shape[1:]))\n",
        "\n",
        "  # Add a dropout layer\n",
        "  model.add(tf.keras.layers.Dropout(dropout))\n",
        "\n",
        "  # Add convolutions, pooling, and more dropout\n",
        "  for _ in range(conv):\n",
        "    model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(reg)))\n",
        "    model.add(tf.keras.layers.MaxPool2D())\n",
        "    model.add(tf.keras.layers.Dropout(dropout))\n",
        "\n",
        "  # Flatten for transition to affine layers\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  # Add affine layers and more dropout\n",
        "  for _ in range(affine):\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(reg)))\n",
        "    model.add(tf.keras.layers.Dropout(dropout))\n",
        "\n",
        "  # Add output layer with softmax activation\n",
        "  model.add(tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.L2(reg)))\n",
        "\n",
        "  # Set loss and optimizer\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr), metrics=['accuracy'])\n",
        "\n",
        "  # Train\n",
        "  model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1, shuffle=True, verbose=1)"
      ],
      "metadata": {
        "id": "4MlQCaaoLURR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "054896e2-77d3-49be-b871-be79be043010"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr: 0.002177 reg: 0.000015\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 10s 5ms/step - loss: 1.1749 - accuracy: 0.6549 - val_loss: 0.1991 - val_accuracy: 0.9590\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3894 - accuracy: 0.8894 - val_loss: 0.1961 - val_accuracy: 0.9555\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3526 - accuracy: 0.9037 - val_loss: 0.1866 - val_accuracy: 0.9598\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3402 - accuracy: 0.9148 - val_loss: 0.1439 - val_accuracy: 0.9693\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3598 - accuracy: 0.9119 - val_loss: 0.1495 - val_accuracy: 0.9682\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.3569 - accuracy: 0.9145 - val_loss: 0.1745 - val_accuracy: 0.9662\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.3713 - accuracy: 0.9144 - val_loss: 0.1705 - val_accuracy: 0.9677\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3634 - accuracy: 0.9190 - val_loss: 0.1809 - val_accuracy: 0.9695\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3662 - accuracy: 0.9201 - val_loss: 0.1706 - val_accuracy: 0.9705\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3596 - accuracy: 0.9227 - val_loss: 0.1568 - val_accuracy: 0.9735\n",
            "lr: 0.000643 reg: 0.000017\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 1.6510 - accuracy: 0.4886 - val_loss: 0.3241 - val_accuracy: 0.9437\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.4977 - accuracy: 0.8451 - val_loss: 0.2092 - val_accuracy: 0.9613\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3412 - accuracy: 0.8982 - val_loss: 0.1670 - val_accuracy: 0.9707\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2667 - accuracy: 0.9216 - val_loss: 0.1289 - val_accuracy: 0.9767\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2218 - accuracy: 0.9353 - val_loss: 0.0970 - val_accuracy: 0.9825\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1970 - accuracy: 0.9433 - val_loss: 0.0869 - val_accuracy: 0.9830\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.1828 - accuracy: 0.9485 - val_loss: 0.0816 - val_accuracy: 0.9837\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1718 - accuracy: 0.9523 - val_loss: 0.0788 - val_accuracy: 0.9832\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1600 - accuracy: 0.9545 - val_loss: 0.0785 - val_accuracy: 0.9830\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1526 - accuracy: 0.9568 - val_loss: 0.0845 - val_accuracy: 0.9835\n",
            "lr: 0.000953 reg: 0.000037\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 1.5351 - accuracy: 0.5479 - val_loss: 0.1905 - val_accuracy: 0.9602\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3411 - accuracy: 0.9044 - val_loss: 0.1066 - val_accuracy: 0.9798\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.2573 - accuracy: 0.9312 - val_loss: 0.1058 - val_accuracy: 0.9777\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2206 - accuracy: 0.9416 - val_loss: 0.1276 - val_accuracy: 0.9797\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2076 - accuracy: 0.9465 - val_loss: 0.1284 - val_accuracy: 0.9765\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2036 - accuracy: 0.9475 - val_loss: 0.0855 - val_accuracy: 0.9855\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.1916 - accuracy: 0.9536 - val_loss: 0.0882 - val_accuracy: 0.9840\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.1895 - accuracy: 0.9531 - val_loss: 0.0802 - val_accuracy: 0.9865\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1836 - accuracy: 0.9548 - val_loss: 0.0796 - val_accuracy: 0.9867\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1732 - accuracy: 0.9589 - val_loss: 0.0807 - val_accuracy: 0.9853\n",
            "lr: 0.000382 reg: 0.000015\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 2.6734 - accuracy: 0.1134 - val_loss: 2.3087 - val_accuracy: 0.1065\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 1.4370 - accuracy: 0.4838 - val_loss: 0.1947 - val_accuracy: 0.9567\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3680 - accuracy: 0.8938 - val_loss: 0.1298 - val_accuracy: 0.9730\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2464 - accuracy: 0.9294 - val_loss: 0.0994 - val_accuracy: 0.9802\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1956 - accuracy: 0.9447 - val_loss: 0.0775 - val_accuracy: 0.9838\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1675 - accuracy: 0.9526 - val_loss: 0.0716 - val_accuracy: 0.9845\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1464 - accuracy: 0.9579 - val_loss: 0.0659 - val_accuracy: 0.9870\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1327 - accuracy: 0.9625 - val_loss: 0.0635 - val_accuracy: 0.9868\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1243 - accuracy: 0.9650 - val_loss: 0.0656 - val_accuracy: 0.9870\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1179 - accuracy: 0.9670 - val_loss: 0.0670 - val_accuracy: 0.9880\n",
            "lr: 0.000914 reg: 0.000091\n",
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 1.5585 - accuracy: 0.5306 - val_loss: 0.2898 - val_accuracy: 0.9482\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.4225 - accuracy: 0.8846 - val_loss: 0.1996 - val_accuracy: 0.9698\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3067 - accuracy: 0.9219 - val_loss: 0.1397 - val_accuracy: 0.9738\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2717 - accuracy: 0.9302 - val_loss: 0.1339 - val_accuracy: 0.9790\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2462 - accuracy: 0.9385 - val_loss: 0.1245 - val_accuracy: 0.9810\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2299 - accuracy: 0.9439 - val_loss: 0.1086 - val_accuracy: 0.9820\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2225 - accuracy: 0.9474 - val_loss: 0.1049 - val_accuracy: 0.9847\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2158 - accuracy: 0.9498 - val_loss: 0.1164 - val_accuracy: 0.9822\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2004 - accuracy: 0.9542 - val_loss: 0.1308 - val_accuracy: 0.9770\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 0.2027 - accuracy: 0.9541 - val_loss: 0.1119 - val_accuracy: 0.9828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Empirically chosen from testing in previous cell\n",
        "lr = 0.000382\n",
        "reg = 0.000015"
      ],
      "metadata": {
        "id": "aIfahgEYWn-q"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the chosen values in a final model\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Input(shape=X_train.shape[1:]))\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(dropout))\n",
        "\n",
        "for _ in range(conv):\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.L2(reg)))\n",
        "  model.add(tf.keras.layers.MaxPool2D())\n",
        "  model.add(tf.keras.layers.Dropout(dropout))\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "for _ in range(affine):\n",
        "  model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(reg)))\n",
        "  model.add(tf.keras.layers.Dropout(dropout))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.L2(reg)))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr), metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_split=0.1, shuffle=True, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmERV5jFWvr6",
        "outputId": "22cacd81-eb9f-4a81-b428-21b2194d8171"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 9s 5ms/step - loss: 2.1006 - accuracy: 0.4008 - val_loss: 0.3650 - val_accuracy: 0.9280\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.4929 - accuracy: 0.8521 - val_loss: 0.1755 - val_accuracy: 0.9633\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.3084 - accuracy: 0.9095 - val_loss: 0.1165 - val_accuracy: 0.9765\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.2346 - accuracy: 0.9313 - val_loss: 0.0907 - val_accuracy: 0.9802\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1943 - accuracy: 0.9454 - val_loss: 0.0826 - val_accuracy: 0.9805\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1704 - accuracy: 0.9520 - val_loss: 0.0658 - val_accuracy: 0.9868\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1499 - accuracy: 0.9576 - val_loss: 0.0695 - val_accuracy: 0.9873\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1404 - accuracy: 0.9604 - val_loss: 0.0705 - val_accuracy: 0.9848\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 8s 5ms/step - loss: 0.1298 - accuracy: 0.9641 - val_loss: 0.0670 - val_accuracy: 0.9853\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 10s 6ms/step - loss: 0.1186 - accuracy: 0.9673 - val_loss: 0.0581 - val_accuracy: 0.9863\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe180720e90>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate it on a set the model has never seen\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O4S-DjQWzn4",
        "outputId": "bc38fc3b-7350-4137-c05b-0a8539aaa092"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.0564 - accuracy: 0.9873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.05641253665089607, 0.9872999787330627]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install tfmot for quantization\n",
        "pip install -q tensorflow-model-optimization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErdbUZBnZYcn",
        "outputId": "bb9c290b-cb46-47d9-e393-e798a35da5e1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█▍                              | 10 kB 24.8 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████                         | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 237 kB 5.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tfmot\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "metadata": {
        "id": "IArcOBKUZ8mG"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantize the model\n",
        "q_aware_model = tfmot.quantization.keras.quantize_model(model)\n",
        "\n",
        "# Set loss and optimizer again\n",
        "q_aware_model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr), metrics=['accuracy'])\n",
        "\n",
        "# Print out model as sanity check\n",
        "q_aware_model.summary()\n",
        "\n",
        "# Perform fine tune training\n",
        "q_aware_model.fit(X_train, y_train, batch_size=32, epochs=3, validation_split=0.1, shuffle=True, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PksmO895aBUJ",
        "outputId": "c334c173-a240-439c-8a01-6eed309b0658"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer_2 (QuantizeL  (None, 28, 28, 1)        3         \n",
            " ayer)                                                           \n",
            "                                                                 \n",
            " quant_dropout_256 (Quantize  (None, 28, 28, 1)        1         \n",
            " WrapperV2)                                                      \n",
            "                                                                 \n",
            " quant_conv2d_129 (QuantizeW  (None, 28, 28, 64)       771       \n",
            " rapperV2)                                                       \n",
            "                                                                 \n",
            " quant_max_pooling2d_129 (Qu  (None, 14, 14, 64)       1         \n",
            " antizeWrapperV2)                                                \n",
            "                                                                 \n",
            " quant_dropout_257 (Quantize  (None, 14, 14, 64)       1         \n",
            " WrapperV2)                                                      \n",
            "                                                                 \n",
            " quant_conv2d_130 (QuantizeW  (None, 14, 14, 64)       37059     \n",
            " rapperV2)                                                       \n",
            "                                                                 \n",
            " quant_max_pooling2d_130 (Qu  (None, 7, 7, 64)         1         \n",
            " antizeWrapperV2)                                                \n",
            "                                                                 \n",
            " quant_dropout_258 (Quantize  (None, 7, 7, 64)         1         \n",
            " WrapperV2)                                                      \n",
            "                                                                 \n",
            " quant_conv2d_131 (QuantizeW  (None, 7, 7, 64)         37059     \n",
            " rapperV2)                                                       \n",
            "                                                                 \n",
            " quant_max_pooling2d_131 (Qu  (None, 3, 3, 64)         1         \n",
            " antizeWrapperV2)                                                \n",
            "                                                                 \n",
            " quant_dropout_259 (Quantize  (None, 3, 3, 64)         1         \n",
            " WrapperV2)                                                      \n",
            "                                                                 \n",
            " quant_flatten_45 (QuantizeW  (None, 576)              1         \n",
            " rapperV2)                                                       \n",
            "                                                                 \n",
            " quant_dense_133 (QuantizeWr  (None, 128)              73861     \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_dropout_260 (Quantize  (None, 128)              1         \n",
            " WrapperV2)                                                      \n",
            "                                                                 \n",
            " quant_dense_134 (QuantizeWr  (None, 128)              16517     \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            " quant_dropout_261 (Quantize  (None, 128)              1         \n",
            " WrapperV2)                                                      \n",
            "                                                                 \n",
            " quant_dense_135 (QuantizeWr  (None, 10)               1295      \n",
            " apperV2)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 166,575\n",
            "Trainable params: 166,154\n",
            "Non-trainable params: 421\n",
            "_________________________________________________________________\n",
            "Epoch 1/4\n",
            "1688/1688 [==============================] - 21s 12ms/step - loss: 0.1277 - accuracy: 0.9665 - val_loss: 0.0535 - val_accuracy: 0.9878\n",
            "Epoch 2/4\n",
            "1688/1688 [==============================] - 20s 12ms/step - loss: 0.1105 - accuracy: 0.9695 - val_loss: 0.0551 - val_accuracy: 0.9880\n",
            "Epoch 3/4\n",
            "1688/1688 [==============================] - 20s 12ms/step - loss: 0.1030 - accuracy: 0.9702 - val_loss: 0.0597 - val_accuracy: 0.9880\n",
            "Epoch 4/4\n",
            "1688/1688 [==============================] - 21s 12ms/step - loss: 0.1031 - accuracy: 0.9715 - val_loss: 0.0660 - val_accuracy: 0.9838\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe20f25e750>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the quantized model\n",
        "q_aware_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnM9J3Yzc_9U",
        "outputId": "fc1e22d3-32b4-4010-93aa-ff68653f8758"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0654 - accuracy: 0.9845\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06544925272464752, 0.984499990940094]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the quantized model to TFLite format for high speed inference\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "quantized_tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtWOUoNQdDoO",
        "outputId": "b486559b-92f4-4a51-e329-588f85475f7e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as dropout_256_layer_call_fn, dropout_256_layer_call_and_return_conditional_losses, conv2d_129_layer_call_fn, conv2d_129_layer_call_and_return_conditional_losses, dropout_257_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmph69e0k1a/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmph69e0k1a/assets\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation method for TFLite model\n",
        "def evaluate_model(interpreter):\n",
        "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
        "\n",
        "  correct_num = 0\n",
        "\n",
        "  # Run predictions on every image in the \"test\" dataset.\n",
        "  for i, test_image in enumerate(X_test):\n",
        "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
        "    # the model's input data format.\n",
        "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
        "    interpreter.set_tensor(input_index, test_image)\n",
        "\n",
        "    # Run inference.\n",
        "    interpreter.invoke()\n",
        "\n",
        "    # Post-processing: remove batch dimension and find the digit with highest\n",
        "    # probability.\n",
        "    output = np.argmax(interpreter.get_tensor(output_index)[0])\n",
        "\n",
        "    correct_num += int(output == y_test[i])\n",
        "\n",
        "  accuracy = correct_num / len(X_test)\n",
        "\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "fSOMB5O0dLsL"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up interpreter and evaluate TFLite model\n",
        "interpreter = tf.lite.Interpreter(model_content=quantized_tflite_model)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Quant TFLite test_accuracy:', test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJK7kTgUdSTe",
        "outputId": "2a43ee56-f0e0-4a76-83c2-02699bdfccea"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quant TFLite test_accuracy: 0.9844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch mounted drive directory into desired directory\n",
        "%cd '/content/gdrive/My Drive/APCSA_MNIST/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTgtfeNZiQSL",
        "outputId": "f3288352-11b1-459f-d500-6a7724bb2b83"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/My Drive/APCSA_MNIST\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import time so that every model save has a unique name\n",
        "import time"
      ],
      "metadata": {
        "id": "3tkpDtBmiWPE"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save TFLite model in drive for download onto computer\n",
        "with open(\"quant-default-\" + str(int(time.time())) + \".tflite\", \"wb\") as f:\n",
        "  f.write(quantized_tflite_model)"
      ],
      "metadata": {
        "id": "liQK-A2nh8c3"
      },
      "execution_count": 68,
      "outputs": []
    }
  ]
}